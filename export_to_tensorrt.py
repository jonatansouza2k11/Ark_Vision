"""
export_to_tensorrt.py
Converte modelo YOLO .pt para TensorRT .engine
"""
from ultralytics import YOLO
import torch

# ConfiguraÃ§Ã£o
MODEL_PT = "yolo_models/yolo11n.pt"  # Modelo PyTorch original
MODEL_ENGINE = "yolo_models/yolo11n.engine"  # SaÃ­da TensorRT
IMGSZ = 640  # Tamanho de entrada (deve bater com inferÃªncia)
DEVICE = 0  # GPU ID (0 para primeira GPU)

print("=" * 70)
print("ğŸš€ Exportando YOLO para TensorRT Engine")
print("=" * 70)
print(f"ğŸ“¦ Modelo origem: {MODEL_PT}")
print(f"ğŸ¯ Destino: {MODEL_ENGINE}")
print(f"ğŸ–¼ï¸  Tamanho imagem: {IMGSZ}x{IMGSZ}")
print(f"ğŸ”§ Device: cuda:{DEVICE}")
print("=" * 70)

# Verificar CUDA
if not torch.cuda.is_available():
    print("âŒ CUDA nÃ£o disponÃ­vel! TensorRT requer GPU NVIDIA.")
    exit(1)

print(f"âœ… GPU detectada: {torch.cuda.get_device_name(DEVICE)}")
print("â³ Iniciando exportaÃ§Ã£o (pode levar 2-5 minutos)...\n")

# Carregar modelo
model = YOLO(MODEL_PT)

# Exportar para TensorRT
# half=True usa FP16 (mais rÃ¡pido, recomendado para inferÃªncia)
# dynamic=False fixa o tamanho de entrada (mais otimizado)
model.export(
    format="engine",
    imgsz=IMGSZ,
    half=True,  # FP16 precision (2x mais rÃ¡pido)
    device=DEVICE,
    dynamic=False,  # Fixed input size (melhor performance)
    simplify=True,
    workspace=4,  # GB de workspace (ajuste conforme sua GPU)
)

print("\n" + "=" * 70)
print("âœ… ExportaÃ§Ã£o concluÃ­da!")
print("=" * 70)
print(f"ğŸ“ Arquivo gerado: {MODEL_ENGINE}")
print("\nğŸ¯ PrÃ³ximos passos:")
print("   1. Atualizar YOLO_MODEL_PATH no .env")
print("   2. Reiniciar o servidor")
print("   3. Testar FPS no dashboard")
print("=" * 70)
