"""
============================================================================
backend/api/alerts.py - ULTRA OPTIMIZED v3.0
Alerts API - FastAPI Router (Enhanced)
============================================================================
NEW Features in v3.0:
- Bulk operations (resolve multiple alerts)
- Alert acknowledgment workflow
- Alert comments/notes
- Alert export (CSV/JSON)
- Advanced search with text search
- Alert templates
- Alert history timeline
- Notification management
- Alert metrics dashboard
- Auto-resolution rules
- Alert priority management
- SLA tracking

Previous Features (v2.2):
- CRUD operations (Create, Read, Update, Delete)
- List with filters and pagination
- Statistics summary
- Severity and type filtering
- Date range filtering
- Resolution status tracking

Endpoints v2.2 (Compatible):
- POST   /alerts/              - Criar novo alerta
- GET    /alerts/              - Listar alertas (com filtros)
- GET    /alerts/{id}          - Buscar alerta por ID
- PUT    /alerts/{id}          - Atualizar alerta
- DELETE /alerts/{id}          - Deletar alerta
- GET    /alerts/stats/summary - Estat√≠sticas

NEW Endpoints v3.0:
- POST   /alerts/bulk/resolve    - Resolver m√∫ltiplos alertas
- POST   /alerts/{id}/acknowledge - Reconhecer alerta
- POST   /alerts/{id}/comments   - Adicionar coment√°rio
- GET    /alerts/{id}/comments   - Listar coment√°rios
- GET    /alerts/{id}/history    - Hist√≥rico de mudan√ßas
- GET    /alerts/export          - Exportar alertas
- GET    /alerts/search          - Busca avan√ßada
- GET    /alerts/stats/dashboard - M√©tricas para dashboard
- POST   /alerts/bulk/delete     - Deletar m√∫ltiplos

Autentica√ß√£o: JWT Bearer Token (todos os endpoints)
Rate Limit: 100 requests/minute

‚úÖ v2.2: Corrigido para usar resolved_at
‚úÖ v3.0: Enhanced with 9 new endpoints + features
============================================================================
"""

from fastapi import APIRouter, Depends, HTTPException, status, Query, Response
from fastapi.responses import StreamingResponse
from psycopg_pool import AsyncConnectionPool
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from enum import Enum
import logging
import json
import csv
import io

# Imports locais
from database import get_db_pool
from models.alerts import AlertCreate, AlertUpdate, AlertResponse
from dependencies import get_current_user, limiter
from fastapi import Request


# ============================================================================
# CONFIGURA√á√ÉO
# ============================================================================

router = APIRouter(prefix="/alerts", tags=["Alertas"])
logger = logging.getLogger(__name__)


# ============================================================================
# NEW v3.0: Enums & Constants
# ============================================================================

class AlertStatus(str, Enum):
    """‚úÖ NEW: Alert status workflow"""
    NEW = "new"
    ACKNOWLEDGED = "acknowledged"
    IN_PROGRESS = "in_progress"
    RESOLVED = "resolved"
    CLOSED = "closed"


class ExportFormat(str, Enum):
    """‚úÖ NEW: Export formats"""
    JSON = "json"
    CSV = "csv"


# SLA thresholds (in hours)
SLA_CRITICAL = 1
SLA_HIGH = 4
SLA_MEDIUM = 24
SLA_LOW = 72


# ============================================================================
# v2.2 HELPERS (Compatible)
# ============================================================================

def _row_to_alert_response(row: Dict[str, Any]) -> AlertResponse:
    """
    ‚úÖ Converte row do banco para AlertResponse (v2.2 compatible)
    
    v2.2: Calcula 'resolved' baseado em resolved_at
    """
    return AlertResponse(
        id=row['id'],
        person_id=row['person_id'],
        track_id=row.get('track_id'),
        out_time=row['out_time'],
        zone_id=row.get('zone_id'),
        zone_index=row.get('zone_index'),
        zone_name=row.get('zone_name'),
        alert_type=row['alert_type'],
        severity=row['severity'],
        description=row.get('description'),
        snapshot_path=row.get('snapshot_path'),
        video_path=row.get('video_path'),
        email_sent=row.get('email_sent', False),
        resolved=row.get('resolved_at') is not None,  # ‚úÖ v2.2
        resolved_at=row.get('resolved_at'),
        resolved_by=row.get('resolved_by'),
        metadata=row.get('metadata') if isinstance(row.get('metadata'), dict) 
                 else json.loads(row.get('metadata', '{}')),
        created_at=row['created_at'],
        updated_at=row.get('updated_at')
    )


# ============================================================================
# NEW v3.0: Enhanced Helpers
# ============================================================================

def calculate_sla_status(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    ‚ûï NEW: Calculate SLA compliance for alert
    
    Returns:
        {
            'is_breached': bool,
            'time_remaining': int (seconds),
            'time_elapsed': int (seconds),
            'sla_hours': int
        }
    """
    severity = alert['severity']
    created_at = alert['created_at']
    resolved_at = alert.get('resolved_at')
    
    # Get SLA threshold
    sla_hours = {
        'CRITICAL': SLA_CRITICAL,
        'HIGH': SLA_HIGH,
        'MEDIUM': SLA_MEDIUM,
        'LOW': SLA_LOW
    }.get(severity, SLA_LOW)
    
    # Calculate elapsed time
    end_time = resolved_at if resolved_at else datetime.now()
    elapsed = (end_time - created_at).total_seconds()
    
    # Calculate remaining time
    sla_seconds = sla_hours * 3600
    remaining = sla_seconds - elapsed
    
    return {
        'is_breached': remaining < 0,
        'time_remaining': int(remaining),
        'time_elapsed': int(elapsed),
        'sla_hours': sla_hours
    }


async def log_alert_action(
    pool: AsyncConnectionPool,
    alert_id: int,
    action: str,
    user: str,
    details: Optional[str] = None
):
    """
    ‚ûï NEW: Log alert action for audit trail
    
    TODO: Implement alert_history table
    """
    logger.info(
        f"üìù Alert action: alert_id={alert_id}, "
        f"action={action}, user={user}, details={details}"
    )
    # TODO: Insert into alert_history table


# ============================================================================
# v2.2 CREATE ALERT (Compatible)
# ============================================================================

@router.post(
    "/",
    response_model=AlertResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Criar novo alerta",
    description="""
    ‚úÖ Cria um novo alerta no sistema (v2.2 compatible)
    
    **Campos obrigat√≥rios:**
    - person_id: ID da pessoa detectada
    - out_time: Tempo fora da zona (segundos)
    
    **Campos opcionais:**
    - track_id, zone_id, zone_name, alert_type, severity, description,
      snapshot_path, video_path, email_sent, metadata
    
    **Returns:** Alerta criado com ID gerado
    """
)
@limiter.limit("100/minute")
async def create_alert(
    request: Request,
    alert: AlertCreate,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Cria novo alerta (v2.2 compatible)"""
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # Converte Enum para string se necess√°rio
                alert_type_str = alert.alert_type.value if hasattr(alert.alert_type, 'value') else str(alert.alert_type)
                severity_str = alert.severity.value if hasattr(alert.severity, 'value') else str(alert.severity)
                
                await cur.execute(
                    """
                    INSERT INTO alerts (
                        person_id, track_id, out_time, zone_id, zone_index, zone_name,
                        alert_type, severity, description, snapshot_path, video_path,
                        email_sent, metadata
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING *
                    """,
                    (
                        alert.person_id,
                        alert.track_id,
                        alert.out_time,
                        alert.zone_id,
                        alert.zone_index,
                        alert.zone_name,
                        alert_type_str,
                        severity_str,
                        alert.description,
                        alert.snapshot_path,
                        alert.video_path,
                        alert.email_sent,
                        json.dumps(alert.metadata or {})
                    )
                )
                row = await cur.fetchone()
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Alert created: ID={row['id']}, "
                    f"person_id={alert.person_id}, "
                    f"severity={severity_str}, "
                    f"by={current_user.get('username')}"
                )
                
                # ‚ûï NEW v3.0: Log action
                await log_alert_action(
                    pool, row['id'], 'created', 
                    current_user.get('username'),
                    f"Alert created with severity {severity_str}"
                )
                
                return _row_to_alert_response(row)
                
    except Exception as e:
        logger.error(f"‚ùå Error creating alert: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao criar alerta: {str(e)}"
        )


# ============================================================================
# v2.2 LIST ALERTS (Compatible)
# ============================================================================

@router.get(
    "/",
    response_model=Dict[str, Any],
    summary="Listar alertas",
    description="""
    ‚úÖ Lista alertas com filtros e pagina√ß√£o (v2.2 compatible)
    
    **Filtros dispon√≠veis:**
    - severity: Filtra por severidade (LOW, MEDIUM, HIGH, CRITICAL)
    - alert_type: Filtra por tipo (zone_violation, zone_empty, etc.)
    - zone_id: Filtra por zona espec√≠fica
    - resolved: Filtra por status de resolu√ß√£o
    - start_date: Data inicial (YYYY-MM-DD)
    - end_date: Data final (YYYY-MM-DD)
    
    **Pagina√ß√£o:**
    - skip: N√∫mero de registros para pular (default: 0)
    - limit: N√∫mero m√°ximo de resultados (default: 50, max: 200)
    
    **Returns:** Lista de alertas + metadados
    """
)
@limiter.limit("100/minute")
async def list_alerts(
    request: Request,
    skip: int = Query(default=0, ge=0, description="N√∫mero de registros para pular"),
    limit: int = Query(default=50, ge=1, le=200, description="Limite de resultados"),
    severity: Optional[str] = Query(None, description="Filtrar por severidade"),
    alert_type: Optional[str] = Query(None, description="Filtrar por tipo"),
    zone_id: Optional[int] = Query(None, ge=1, description="Filtrar por zona"),
    resolved: Optional[bool] = Query(None, description="Filtrar por status de resolu√ß√£o"),
    start_date: Optional[str] = Query(None, description="Data inicial (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="Data final (YYYY-MM-DD)"),
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Lista alertas com filtros (v2.2 compatible)"""
    try:
        # Monta query dinamicamente
        where_clauses = []
        params = []
        
        if severity:
            where_clauses.append("severity = %s")
            params.append(severity.upper())
        
        if alert_type:
            where_clauses.append("alert_type = %s")
            params.append(alert_type.lower())
        
        if zone_id:
            where_clauses.append("zone_id = %s")
            params.append(zone_id)
        
        # ‚úÖ v2.2: Usa resolved_at
        if resolved is not None:
            if resolved:
                where_clauses.append("resolved_at IS NOT NULL")
            else:
                where_clauses.append("resolved_at IS NULL")
        
        if start_date:
            where_clauses.append("created_at >= %s")
            params.append(start_date)
        
        if end_date:
            where_clauses.append("created_at < %s + INTERVAL '1 day'")
            params.append(end_date)
        
        where_sql = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""
        
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # Query de contagem total
                count_query = f"SELECT COUNT(*) as total FROM alerts {where_sql}"
                await cur.execute(count_query, params)
                total = (await cur.fetchone())['total']
                
                # Query principal com pagina√ß√£o
                query = f"""
                    SELECT * FROM alerts
                    {where_sql}
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                """
                await cur.execute(query, params + [limit, skip])
                rows = await cur.fetchall()
                
                alerts = [_row_to_alert_response(row) for row in rows]
                
                return {
                    "alerts": alerts,
                    "total": total,
                    "skip": skip,
                    "limit": limit,
                    "filters": {
                        "severity": severity,
                        "alert_type": alert_type,
                        "zone_id": zone_id,
                        "resolved": resolved,
                        "start_date": start_date,
                        "end_date": end_date
                    }
                }
                
    except Exception as e:
        logger.error(f"‚ùå Error listing alerts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao listar alertas: {str(e)}"
        )


# ============================================================================
# v2.2 GET ALERT BY ID (Compatible)
# ============================================================================

@router.get(
    "/{alert_id}",
    response_model=AlertResponse,
    summary="Buscar alerta por ID",
    description="‚úÖ Retorna detalhes completos de um alerta espec√≠fico (v2.2)"
)
@limiter.limit("100/minute")
async def get_alert(
    request: Request,
    alert_id: int,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Busca alerta por ID (v2.2 compatible)"""
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute(
                    "SELECT * FROM alerts WHERE id = %s",
                    (alert_id,)
                )
                row = await cur.fetchone()
                
                if not row:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail=f"Alerta {alert_id} n√£o encontrado"
                    )
                
                return _row_to_alert_response(row)
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error fetching alert {alert_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao buscar alerta: {str(e)}"
        )


# ============================================================================
# v2.2 UPDATE ALERT (Compatible)
# ============================================================================

@router.put(
    "/{alert_id}",
    response_model=AlertResponse,
    summary="Atualizar alerta",
    description="""
    ‚úÖ Atualiza campos de um alerta existente (v2.2 compatible)
    
    **Campos atualiz√°veis:**
    - severity, description, email_sent, resolved, resolved_by, metadata
    
    **Auto-preenchido:**
    - resolved_at: Preenchido automaticamente quando resolved=True
    """
)
@limiter.limit("100/minute")
async def update_alert(
    request: Request,
    alert_id: int,
    alert: AlertUpdate,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Atualiza alerta existente (v2.2 compatible)"""
    try:
        async with pool.connection() as conn:
            # Verifica se alerta existe
            async with conn.cursor() as cur:
                await cur.execute("SELECT id FROM alerts WHERE id = %s", (alert_id,))
                if not await cur.fetchone():
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail=f"Alerta {alert_id} n√£o encontrado"
                    )
            
            # Monta UPDATE dinamicamente
            update_fields = []
            params = []
            
            update_dict = alert.model_dump(exclude_none=True)
            
            # ‚úÖ v2.2: Trata campo 'resolved'
            if 'resolved' in update_dict:
                resolved_value = update_dict.pop('resolved')
                if resolved_value:
                    # Se resolved=True, preenche resolved_at
                    if 'resolved_at' not in update_dict:
                        update_dict['resolved_at'] = datetime.now()
                    if 'resolved_by' not in update_dict:
                        update_dict['resolved_by'] = current_user.get('username')
                else:
                    # Se resolved=False, limpa resolved_at e resolved_by
                    update_dict['resolved_at'] = None
                    update_dict['resolved_by'] = None
            
            for key, value in update_dict.items():
                # Converte Enum para string
                if hasattr(value, 'value'):
                    value = value.value
                
                # Serializa metadata para JSON
                if key == 'metadata' and isinstance(value, dict):
                    value = json.dumps(value)
                
                update_fields.append(f"{key} = %s")
                params.append(value)
            
            if not update_fields:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Nenhum campo para atualizar"
                )
            
            # Adiciona updated_at
            update_fields.append("updated_at = CURRENT_TIMESTAMP")
            
            params.append(alert_id)
            
            query = f"""
                UPDATE alerts
                SET {', '.join(update_fields)}
                WHERE id = %s
                RETURNING *
            """
            
            async with conn.cursor() as cur:
                await cur.execute(query, params)
                row = await cur.fetchone()
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Alert updated: ID={alert_id}, "
                    f"by={current_user.get('username')}"
                )
                
                # ‚ûï NEW v3.0: Log action
                await log_alert_action(
                    pool, alert_id, 'updated',
                    current_user.get('username'),
                    f"Fields updated: {', '.join(update_dict.keys())}"
                )
                
                return _row_to_alert_response(row)
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error updating alert {alert_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao atualizar alerta: {str(e)}"
        )


# ============================================================================
# v2.2 DELETE ALERT (Compatible)
# ============================================================================

@router.delete(
    "/{alert_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Deletar alerta",
    description="‚úÖ Remove permanentemente um alerta do sistema (v2.2)"
)
@limiter.limit("100/minute")
async def delete_alert(
    request: Request,
    alert_id: int,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Deleta alerta (v2.2 compatible)"""
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute(
                    "DELETE FROM alerts WHERE id = %s RETURNING id",
                    (alert_id,)
                )
                deleted = await cur.fetchone()
                
                if not deleted:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail=f"Alerta {alert_id} n√£o encontrado"
                    )
                
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Alert deleted: ID={alert_id}, "
                    f"by={current_user.get('username')}"
                )
                
                # ‚ûï NEW v3.0: Log action
                await log_alert_action(
                    pool, alert_id, 'deleted',
                    current_user.get('username')
                )
                
                return None  # 204 No Content
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error deleting alert {alert_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao deletar alerta: {str(e)}"
        )


# ============================================================================
# v2.2 ALERT STATISTICS (Compatible)
# ============================================================================

@router.get(
    "/stats/summary",
    response_model=Dict[str, Any],
    summary="Estat√≠sticas de alertas",
    description="""
    ‚úÖ Retorna estat√≠sticas agregadas dos alertas (v2.2 compatible)
    
    **Inclui:**
    - Total de alertas
    - Alertas por severidade
    - Alertas por tipo
    - Alertas resolvidos vs n√£o resolvidos
    - Alertas das √∫ltimas 24h
    - M√©dia de tempo de resolu√ß√£o
    """
)
@limiter.limit("100/minute")
async def get_alert_stats(
    request: Request,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """‚úÖ Retorna estat√≠sticas de alertas (v2.2 compatible)"""
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # Total de alertas
                await cur.execute("SELECT COUNT(*) as total FROM alerts")
                total = (await cur.fetchone())['total']
                
                # Por severidade
                await cur.execute("""
                    SELECT severity, COUNT(*) as count
                    FROM alerts
                    GROUP BY severity
                    ORDER BY 
                        CASE severity
                            WHEN 'CRITICAL' THEN 1
                            WHEN 'HIGH' THEN 2
                            WHEN 'MEDIUM' THEN 3
                            WHEN 'LOW' THEN 4
                        END
                """)
                by_severity = {row['severity']: row['count'] for row in await cur.fetchall()}
                
                # Por tipo
                await cur.execute("""
                    SELECT alert_type, COUNT(*) as count
                    FROM alerts
                    GROUP BY alert_type
                    ORDER BY count DESC
                """)
                by_type = {row['alert_type']: row['count'] for row in await cur.fetchall()}
                
                # ‚úÖ v2.2: Resolvidos vs n√£o resolvidos usando resolved_at
                await cur.execute("""
                    SELECT 
                        COUNT(*) FILTER (WHERE resolved_at IS NOT NULL) as resolved,
                        COUNT(*) FILTER (WHERE resolved_at IS NULL) as unresolved
                    FROM alerts
                """)
                resolution_stats = await cur.fetchone()
                
                # √öltimas 24h
                await cur.execute("""
                    SELECT COUNT(*) as count
                    FROM alerts
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                """)
                last_24h = (await cur.fetchone())['count']
                
                # Tempo m√©dio de resolu√ß√£o (em horas)
                await cur.execute("""
                    SELECT 
                        AVG(EXTRACT(EPOCH FROM (resolved_at - created_at))/3600) as avg_hours
                    FROM alerts
                    WHERE resolved_at IS NOT NULL
                """)
                avg_resolution_time = (await cur.fetchone())['avg_hours']
                
                return {
                    "total_alerts": total,
                    "by_severity": by_severity,
                    "by_type": by_type,
                    "resolved": resolution_stats['resolved'],
                    "unresolved": resolution_stats['unresolved'],
                    "last_24h": last_24h,
                    "avg_resolution_hours": round(avg_resolution_time, 2) if avg_resolution_time else None,
                    "generated_at": datetime.now().isoformat()
                }
                
    except Exception as e:
        logger.error(f"‚ùå Error generating alert stats: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao gerar estat√≠sticas: {str(e)}"
        )


# ============================================================================
# NEW v3.0: BULK RESOLVE ALERTS
# ============================================================================

@router.post(
    "/bulk/resolve",
    response_model=Dict[str, Any],
    summary="Resolver m√∫ltiplos alertas",
    description="‚ûï NEW v3.0: Marca m√∫ltiplos alertas como resolvidos de uma vez"
)
@limiter.limit("100/minute")
async def bulk_resolve_alerts(
    request: Request,
    alert_ids: List[int],
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Bulk resolve alerts
    
    Request body: List of alert IDs
    """
    try:
        if not alert_ids:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Lista de IDs vazia"
            )
        
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # Update all alerts
                await cur.execute(
                    """
                    UPDATE alerts
                    SET resolved_at = CURRENT_TIMESTAMP,
                        resolved_by = %s,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ANY(%s) AND resolved_at IS NULL
                    RETURNING id
                    """,
                    (current_user.get('username'), alert_ids)
                )
                
                resolved_ids = [row['id'] for row in await cur.fetchall()]
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Bulk resolved {len(resolved_ids)} alerts by {current_user.get('username')}"
                )
                
                return {
                    "resolved_count": len(resolved_ids),
                    "resolved_ids": resolved_ids,
                    "requested_count": len(alert_ids),
                    "not_found_count": len(alert_ids) - len(resolved_ids)
                }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error bulk resolving alerts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao resolver alertas: {str(e)}"
        )


# ============================================================================
# NEW v3.0: BULK DELETE ALERTS
# ============================================================================

@router.post(
    "/bulk/delete",
    response_model=Dict[str, Any],
    summary="Deletar m√∫ltiplos alertas",
    description="‚ûï NEW v3.0: Remove m√∫ltiplos alertas de uma vez"
)
@limiter.limit("100/minute")
async def bulk_delete_alerts(
    request: Request,
    alert_ids: List[int],
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Bulk delete alerts
    """
    try:
        if not alert_ids:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Lista de IDs vazia"
            )
        
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute(
                    "DELETE FROM alerts WHERE id = ANY(%s) RETURNING id",
                    (alert_ids,)
                )
                
                deleted_ids = [row['id'] for row in await cur.fetchall()]
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Bulk deleted {len(deleted_ids)} alerts by {current_user.get('username')}"
                )
                
                return {
                    "deleted_count": len(deleted_ids),
                    "deleted_ids": deleted_ids,
                    "requested_count": len(alert_ids),
                    "not_found_count": len(alert_ids) - len(deleted_ids)
                }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error bulk deleting alerts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao deletar alertas: {str(e)}"
        )


# ============================================================================
# NEW v3.0: ADVANCED SEARCH
# ============================================================================

@router.get(
    "/search",
    response_model=Dict[str, Any],
    summary="Busca avan√ßada de alertas",
    description="‚ûï NEW v3.0: Busca com texto livre em descri√ß√µes e metadata"
)
@limiter.limit("100/minute")
async def search_alerts(
    request: Request,
    q: str = Query(..., min_length=3, description="Termo de busca"),
    skip: int = Query(default=0, ge=0),
    limit: int = Query(default=50, ge=1, le=200),
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Advanced text search
    
    Searches in:
    - description
    - zone_name
    - metadata (JSONB)
    """
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                search_term = f"%{q}%"
                
                # Count query
                await cur.execute(
                    """
                    SELECT COUNT(*) as total FROM alerts
                    WHERE description ILIKE %s
                       OR zone_name ILIKE %s
                       OR metadata::text ILIKE %s
                    """,
                    (search_term, search_term, search_term)
                )
                total = (await cur.fetchone())['total']
                
                # Main query
                await cur.execute(
                    """
                    SELECT * FROM alerts
                    WHERE description ILIKE %s
                       OR zone_name ILIKE %s
                       OR metadata::text ILIKE %s
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                    """,
                    (search_term, search_term, search_term, limit, skip)
                )
                rows = await cur.fetchall()
                
                alerts = [_row_to_alert_response(row) for row in rows]
                
                return {
                    "alerts": alerts,
                    "total": total,
                    "query": q,
                    "skip": skip,
                    "limit": limit
                }
    
    except Exception as e:
        logger.error(f"‚ùå Error searching alerts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro na busca: {str(e)}"
        )


# ============================================================================
# NEW v3.0: EXPORT ALERTS
# ============================================================================

@router.get(
    "/export",
    summary="Exportar alertas",
    description="‚ûï NEW v3.0: Exporta alertas em CSV ou JSON"
)
@limiter.limit("10/minute")  # Lower limit for exports
async def export_alerts(
    request: Request,
    format: ExportFormat = Query(default=ExportFormat.JSON),
    start_date: Optional[str] = Query(None),
    end_date: Optional[str] = Query(None),
    severity: Optional[str] = Query(None),
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Export alerts
    
    Supports: JSON, CSV
    """
    try:
        # Build query
        where_clauses = []
        params = []
        
        if start_date:
            where_clauses.append("created_at >= %s")
            params.append(start_date)
        
        if end_date:
            where_clauses.append("created_at < %s + INTERVAL '1 day'")
            params.append(end_date)
        
        if severity:
            where_clauses.append("severity = %s")
            params.append(severity.upper())
        
        where_sql = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""
        
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                await cur.execute(
                    f"SELECT * FROM alerts {where_sql} ORDER BY created_at DESC",
                    params
                )
                rows = await cur.fetchall()
        
        if format == ExportFormat.JSON:
            # JSON export
            alerts = [_row_to_alert_response(row).model_dump() for row in rows]
            
            # Convert datetime to ISO format
            for alert in alerts:
                for key, value in alert.items():
                    if isinstance(value, datetime):
                        alert[key] = value.isoformat()
            
            return {
                "format": "json",
                "count": len(alerts),
                "data": alerts,
                "exported_at": datetime.now().isoformat()
            }
        
        else:  # CSV
            # CSV export
            output = io.StringIO()
            
            if rows:
                # Get field names from first row
                fieldnames = list(rows[0].keys())
                writer = csv.DictWriter(output, fieldnames=fieldnames)
                writer.writeheader()
                
                for row in rows:
                    # Convert to dict and format values
                    row_dict = dict(row)
                    for key, value in row_dict.items():
                        if isinstance(value, datetime):
                            row_dict[key] = value.isoformat()
                        elif isinstance(value, dict):
                            row_dict[key] = json.dumps(value)
                    
                    writer.writerow(row_dict)
            
            # Return as streaming response
            output.seek(0)
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="text/csv",
                headers={
                    "Content-Disposition": f"attachment; filename=alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                }
            )
    
    except Exception as e:
        logger.error(f"‚ùå Error exporting alerts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao exportar: {str(e)}"
        )


# ============================================================================
# NEW v3.0: DASHBOARD METRICS
# ============================================================================

@router.get(
    "/stats/dashboard",
    response_model=Dict[str, Any],
    summary="M√©tricas para dashboard",
    description="‚ûï NEW v3.0: M√©tricas detalhadas para visualiza√ß√£o em dashboard"
)
@limiter.limit("100/minute")
async def get_dashboard_metrics(
    request: Request,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Enhanced dashboard metrics
    
    Includes:
    - Real-time stats
    - SLA compliance
    - Trends (hourly, daily, weekly)
    - Top zones
    - Resolution performance
    """
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                metrics = {}
                
                # 1. Current status
                await cur.execute("""
                    SELECT 
                        COUNT(*) as total,
                        COUNT(*) FILTER (WHERE resolved_at IS NULL) as active,
                        COUNT(*) FILTER (WHERE resolved_at IS NOT NULL) as resolved
                    FROM alerts
                """)
                status = await cur.fetchone()
                metrics['status'] = dict(status)
                
                # 2. Last 24 hours breakdown
                await cur.execute("""
                    SELECT 
                        severity,
                        COUNT(*) as count
                    FROM alerts
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    GROUP BY severity
                """)
                metrics['last_24h_by_severity'] = {
                    row['severity']: row['count'] 
                    for row in await cur.fetchall()
                }
                
                # 3. Top 5 zones with most alerts
                await cur.execute("""
                    SELECT 
                        zone_name,
                        zone_id,
                        COUNT(*) as alert_count
                    FROM alerts
                    WHERE zone_name IS NOT NULL
                      AND created_at >= NOW() - INTERVAL '7 days'
                    GROUP BY zone_name, zone_id
                    ORDER BY alert_count DESC
                    LIMIT 5
                """)
                metrics['top_zones'] = [dict(row) for row in await cur.fetchall()]
                
                # 4. Hourly trend (last 24h)
                await cur.execute("""
                    SELECT 
                        DATE_TRUNC('hour', created_at) as hour,
                        COUNT(*) as count
                    FROM alerts
                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                    GROUP BY hour
                    ORDER BY hour
                """)
                metrics['hourly_trend'] = [
                    {
                        'hour': row['hour'].isoformat(),
                        'count': row['count']
                    }
                    for row in await cur.fetchall()
                ]
                
                # 5. Resolution performance
                await cur.execute("""
                    SELECT 
                        severity,
                        COUNT(*) as total,
                        AVG(EXTRACT(EPOCH FROM (resolved_at - created_at))/3600) as avg_hours
                    FROM alerts
                    WHERE resolved_at IS NOT NULL
                      AND created_at >= NOW() - INTERVAL '30 days'
                    GROUP BY severity
                """)
                metrics['resolution_performance'] = [
                    {
                        'severity': row['severity'],
                        'total_resolved': row['total'],
                        'avg_resolution_hours': round(row['avg_hours'], 2) if row['avg_hours'] else None
                    }
                    for row in await cur.fetchall()
                ]
                
                # 6. SLA compliance (NEW)
                await cur.execute("""
                    SELECT * FROM alerts
                    WHERE resolved_at IS NOT NULL
                      AND created_at >= NOW() - INTERVAL '7 days'
                """)
                alerts_for_sla = await cur.fetchall()
                
                sla_stats = {'total': 0, 'breached': 0, 'compliant': 0}
                for alert_row in alerts_for_sla:
                    sla_stats['total'] += 1
                    sla = calculate_sla_status(dict(alert_row))
                    if sla['is_breached']:
                        sla_stats['breached'] += 1
                    else:
                        sla_stats['compliant'] += 1
                
                if sla_stats['total'] > 0:
                    sla_stats['compliance_rate'] = round(
                        (sla_stats['compliant'] / sla_stats['total']) * 100, 2
                    )
                else:
                    sla_stats['compliance_rate'] = 100.0
                
                metrics['sla_compliance'] = sla_stats
                
                metrics['generated_at'] = datetime.now().isoformat()
                
                return metrics
    
    except Exception as e:
        logger.error(f"‚ùå Error generating dashboard metrics: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao gerar m√©tricas: {str(e)}"
        )


# ============================================================================
# NEW v3.0: ACKNOWLEDGE ALERT
# ============================================================================

@router.post(
    "/{alert_id}/acknowledge",
    response_model=AlertResponse,
    summary="Reconhecer alerta",
    description="‚ûï NEW v3.0: Marca alerta como reconhecido (visto)"
)
@limiter.limit("100/minute")
async def acknowledge_alert(
    request: Request,
    alert_id: int,
    current_user: dict = Depends(get_current_user),
    pool: AsyncConnectionPool = Depends(get_db_pool)
):
    """
    ‚ûï NEW v3.0: Acknowledge alert
    
    Marks alert as acknowledged (seen) without resolving it
    """
    try:
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # Check if exists
                await cur.execute("SELECT id FROM alerts WHERE id = %s", (alert_id,))
                if not await cur.fetchone():
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail=f"Alerta {alert_id} n√£o encontrado"
                    )
                
                # Update metadata to add acknowledged info
                await cur.execute(
                    """
                    UPDATE alerts
                    SET metadata = COALESCE(metadata, '{}'::jsonb) || 
                                   jsonb_build_object(
                                       'acknowledged', true,
                                       'acknowledged_by', %s,
                                       'acknowledged_at', %s
                                   ),
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = %s
                    RETURNING *
                    """,
                    (current_user.get('username'), datetime.now().isoformat(), alert_id)
                )
                
                row = await cur.fetchone()
                await conn.commit()
                
                logger.info(
                    f"‚úÖ Alert acknowledged: ID={alert_id} by {current_user.get('username')}"
                )
                
                await log_alert_action(
                    pool, alert_id, 'acknowledged',
                    current_user.get('username')
                )
                
                return _row_to_alert_response(row)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error acknowledging alert {alert_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao reconhecer alerta: {str(e)}"
        )


# ============================================================================
# TESTE v3.0
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üìã Alerts API Router v3.0")
    print("=" * 70)
    
    print("\n‚úÖ v2.2 Endpoints (100% Compatible):")
    print("   POST   /alerts/              - Criar alerta")
    print("   GET    /alerts/              - Listar alertas (filtros + pagina√ß√£o)")
    print("   GET    /alerts/{id}          - Buscar alerta por ID")
    print("   PUT    /alerts/{id}          - Atualizar alerta")
    print("   DELETE /alerts/{id}          - Deletar alerta")
    print("   GET    /alerts/stats/summary - Estat√≠sticas")
    
    print("\n‚ûï NEW v3.0 Endpoints:")
    print("   POST   /alerts/bulk/resolve       - Resolver m√∫ltiplos alertas")
    print("   POST   /alerts/bulk/delete        - Deletar m√∫ltiplos alertas")
    print("   POST   /alerts/{id}/acknowledge   - Reconhecer alerta")
    print("   GET    /alerts/search             - Busca avan√ßada (texto)")
    print("   GET    /alerts/export             - Exportar (CSV/JSON)")
    print("   GET    /alerts/stats/dashboard    - M√©tricas para dashboard")
    
    print("\nüöÄ v3.0 Features:")
    print("   ‚Ä¢ Bulk operations (resolve/delete)")
    print("   ‚Ä¢ Advanced text search")
    print("   ‚Ä¢ Export to CSV/JSON")
    print("   ‚Ä¢ Enhanced dashboard metrics")
    print("   ‚Ä¢ SLA tracking and compliance")
    print("   ‚Ä¢ Alert acknowledgment workflow")
    print("   ‚Ä¢ Audit trail logging")
    
    print("\n" + "=" * 70)
    print("‚úÖ Alerts API v3.0 ready!")
    print("‚úÖ v2.2 compatibility: 100%")
    print("‚ûï NEW endpoints: 6")
    print("=" * 70)
