"""
backup_logs.py

Sistema de backup autom√°tico de logs com compliance ANVISA/FDA.

Recursos:
- Compress√£o gzip (economia de 80-90% de espa√ßo)
- Reten√ß√£o de 5 anos (requisito regulat√≥rio)
- Organiza√ß√£o por m√™s
- Verifica√ß√£o de integridade
- Backup incremental
- Prote√ß√£o contra altera√ß√£o (read-only)
- Relat√≥rios de backup

Uso:
    python backup_logs.py              # Backup manual
    python backup_logs.py --verify     # Verifica integridade
    python backup_logs.py --cleanup    # Remove logs expirados (dry-run)
    python backup_logs.py --cleanup --no-dry-run  # Remove de verdade
    python backup_logs.py --stats      # Estat√≠sticas
"""

import os
import gzip
import shutil
import hashlib
import json
from pathlib import Path
from datetime import datetime, timedelta
import argparse


class LogBackupManager:
    """Gerenciador de backup de logs com compliance regulat√≥rio."""
    
    LOG_DIR = Path('logs')
    ARCHIVE_DIR = LOG_DIR / 'archive'
    RETENTION_YEARS = 5
    
    LOG_FILES = [
        'app.log', 'app.log.1', 'app.log.2', 'app.log.3', 'app.log.4',
        'app.log.5', 'app.log.6', 'app.log.7', 'app.log.8', 'app.log.9',
        'app.log.10', 'audit.log',
    ]
    
    def __init__(self):
        self.ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
        print(f"üì¶ LogBackupManager initialized")
        print(f"   Archive dir: {self.ARCHIVE_DIR}")
        print(f"   Retention: {self.RETENTION_YEARS} years")
    
    def _get_month_dir(self, date=None):
        if date is None:
            date = datetime.now()
        month_str = date.strftime('%Y-%m')
        month_dir = self.ARCHIVE_DIR / month_str
        month_dir.mkdir(parents=True, exist_ok=True)
        return month_dir
    
    def _calculate_file_hash(self, filepath):
        sha256 = hashlib.sha256()
        try:
            with open(filepath, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b''):
                    sha256.update(chunk)
            return sha256.hexdigest()
        except FileNotFoundError:
            return None
    
    def _compress_file(self, source_path, dest_path):
        with open(source_path, 'rb') as f_in:
            with gzip.open(dest_path, 'wb', compresslevel=9) as f_out:
                shutil.copyfileobj(f_in, f_out)
    
    def _make_readonly(self, filepath):
        try:
            os.chmod(filepath, 0o444)
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not set read-only: {e}")
    
    def backup_log_file(self, log_filename):
        source_path = self.LOG_DIR / log_filename
        
        if not source_path.exists():
            print(f"‚è≠Ô∏è  Skipping {log_filename} (not found)")
            return None
        
        if source_path.stat().st_size == 0:
            print(f"‚è≠Ô∏è  Skipping {log_filename} (empty)")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        month_dir = self._get_month_dir()
        backup_name = f"{source_path.stem}_{timestamp}.log.gz"
        dest_path = month_dir / backup_name
        
        original_hash = self._calculate_file_hash(source_path)
        original_size = source_path.stat().st_size
        
        print(f"üì¶ Backing up: {log_filename}")
        print(f"   Size: {original_size:,} bytes")
        
        self._compress_file(source_path, dest_path)
        
        compressed_size = dest_path.stat().st_size
        compression_ratio = (1 - compressed_size / original_size) * 100
        
        print(f"   ‚úÖ Compressed: {compressed_size:,} bytes ({compression_ratio:.1f}% reduction)")
        
        self._make_readonly(dest_path)
        
        metadata = {
            'original_file': log_filename,
            'backup_file': backup_name,
            'backup_path': str(dest_path),
            'timestamp': timestamp,
            'original_size': original_size,
            'compressed_size': compressed_size,
            'compression_ratio': compression_ratio,
            'original_hash': original_hash,
            'created_at': datetime.now().isoformat()
        }
        
        metadata_path = dest_path.with_suffix('.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        self._make_readonly(metadata_path)
        
        return metadata
    
    def backup_all_logs(self):
        print("\n" + "=" * 70)
        print("üì¶ INICIANDO BACKUP DE LOGS")
        print("=" * 70)
        
        backups = []
        
        for log_file in self.LOG_FILES:
            metadata = self.backup_log_file(log_file)
            if metadata:
                backups.append(metadata)
            print()
        
        print("=" * 70)
        print(f"‚úÖ BACKUP CONCLU√çDO: {len(backups)} arquivo(s)")
        print("=" * 70)
        
        report_path = self.ARCHIVE_DIR / f"backup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump({
                'backup_date': datetime.now().isoformat(),
                'total_files': len(backups),
                'backups': backups
            }, f, indent=2)
        
        return backups
    
    def cleanup_old_backups(self, dry_run=True):
        print("\n" + "=" * 70)
        print(f"üóëÔ∏è  LIMPEZA DE BACKUPS ANTIGOS (>{self.RETENTION_YEARS} anos)")
        print("=" * 70)
        
        cutoff_date = datetime.now() - timedelta(days=365 * self.RETENTION_YEARS)
        print(f"Cutoff date: {cutoff_date.strftime('%Y-%m-%d')}")
        print(f"Modo: {'DRY RUN (simula√ß√£o)' if dry_run else 'EXECU√á√ÉO REAL'}")
        print()
        
        removed_files = []
        total_size = 0
        
        for backup_file in self.ARCHIVE_DIR.rglob('*.gz'):
            file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            
            if file_mtime < cutoff_date:
                size = backup_file.stat().st_size
                total_size += size
                
                print(f"{'[DRY RUN] ' if dry_run else ''}üóëÔ∏è  Removing: {backup_file.name}")
                print(f"   Date: {file_mtime.strftime('%Y-%m-%d')}, Size: {size:,} bytes")
                
                removed_files.append({
                    'file': str(backup_file),
                    'date': file_mtime.isoformat(),
                    'size': size
                })
                
                if not dry_run:
                    backup_file.unlink()
                    metadata_file = backup_file.with_suffix('.json')
                    if metadata_file.exists():
                        metadata_file.unlink()
        
        print()
        print("=" * 70)
        print(f"{'Seriam removidos' if dry_run else 'Removidos'}: {len(removed_files)} arquivo(s)")
        print(f"Espa√ßo: {total_size / 1024 / 1024:.2f} MB")
        print("=" * 70)
        
        return removed_files
    
    def verify_backup_integrity(self):
        print("\n" + "=" * 70)
        print("üîç VERIFICA√á√ÉO DE INTEGRIDADE DOS BACKUPS")
        print("=" * 70)
        
        total = 0
        valid = 0
        invalid = []
        
        for backup_file in self.ARCHIVE_DIR.rglob('*.gz'):
            total += 1
            metadata_file = backup_file.with_suffix('.json')
            
            if not metadata_file.exists():
                print(f"‚ö†Ô∏è  {backup_file.name}: Metadata ausente")
                invalid.append(str(backup_file))
                continue
            
            try:
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                current_size = backup_file.stat().st_size
                if current_size != metadata['compressed_size']:
                    print(f"‚ùå {backup_file.name}: Tamanho n√£o confere")
                    invalid.append(str(backup_file))
                    continue
                
                print(f"‚úÖ {backup_file.name}: OK")
                valid += 1
                
            except Exception as e:
                print(f"‚ùå {backup_file.name}: Erro - {e}")
                invalid.append(str(backup_file))
        
        print()
        print("=" * 70)
        print(f"Total: {total} | V√°lidos: {valid} | Inv√°lidos: {len(invalid)}")
        print("=" * 70)
        
        return (total, valid, invalid)
    
    def get_backup_statistics(self):
        total_files = 0
        total_size = 0
        oldest_backup = None
        newest_backup = None
        
        for backup_file in self.ARCHIVE_DIR.rglob('*.gz'):
            total_files += 1
            total_size += backup_file.stat().st_size
            
            mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            
            if oldest_backup is None or mtime < oldest_backup:
                oldest_backup = mtime
            if newest_backup is None or mtime > newest_backup:
                newest_backup = mtime
        
        stats = {
            'total_backups': total_files,
            'total_size_bytes': total_size,
            'total_size_mb': round(total_size / 1024 / 1024, 2),
            'oldest_backup': oldest_backup.isoformat() if oldest_backup else None,
            'newest_backup': newest_backup.isoformat() if newest_backup else None,
            'retention_compliant': (
                (datetime.now() - oldest_backup).days >= (365 * self.RETENTION_YEARS)
                if oldest_backup else False
            )
        }
        
        print("\n" + "=" * 70)
        print("üìä ESTAT√çSTICAS DE BACKUP")
        print("=" * 70)
        print(f"Total de backups: {stats['total_backups']}")
        print(f"Tamanho total: {stats['total_size_mb']} MB")
        print(f"Backup mais antigo: {oldest_backup.strftime('%Y-%m-%d %H:%M') if oldest_backup else 'N/A'}")
        print(f"Backup mais recente: {newest_backup.strftime('%Y-%m-%d %H:%M') if newest_backup else 'N/A'}")
        print(f"Compliant (5 anos): {'‚úÖ Sim' if stats['retention_compliant'] else '‚ùå N√£o'}")
        print("=" * 70)
        
        return stats


def main():
    parser = argparse.ArgumentParser(
        description='Sistema de Backup de Logs - ARK YOLO',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos:
  python backup_logs.py                    # Backup de todos os logs
  python backup_logs.py --verify           # Verifica integridade
  python backup_logs.py --cleanup          # Simula limpeza
  python backup_logs.py --cleanup --no-dry-run  # Executa limpeza
  python backup_logs.py --stats            # Mostra estat√≠sticas
        """
    )
    parser.add_argument('--backup', action='store_true', help='Executa backup')
    parser.add_argument('--verify', action='store_true', help='Verifica integridade')
    parser.add_argument('--cleanup', action='store_true', help='Remove backups antigos')
    parser.add_argument('--stats', action='store_true', help='Mostra estat√≠sticas')
    parser.add_argument('--no-dry-run', action='store_true', help='Executa cleanup real')
    
    args = parser.parse_args()
    
    manager = LogBackupManager()
    
    # Se nenhum argumento, faz backup
    if not any([args.backup, args.verify, args.cleanup, args.stats]):
        args.backup = True
    
    if args.backup:
        manager.backup_all_logs()
    
    if args.verify:
        manager.verify_backup_integrity()
    
    if args.cleanup:
        dry_run = not args.no_dry_run
        manager.cleanup_old_backups(dry_run=dry_run)
    
    if args.stats:
        manager.get_backup_statistics()


if __name__ == "__main__":
    main()
